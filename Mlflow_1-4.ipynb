{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir_Miy2BpReR"
   },
   "source": [
    " **Module 1.4: MLflow UI Deep Dive** is now ready. It walks learners through:\n",
    "## üéØ **Learning Objectives**\n",
    "\n",
    "### 1Ô∏è‚É£ **Access and Inspect MLflow Runs Programmatically**\n",
    "\n",
    "* **What it means:**\n",
    "  Using MLflow‚Äôs Python API to retrieve details of past runs, including metrics, parameters, and logged artifacts directly from code.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Import MLflow:\n",
    "\n",
    "    ```python\n",
    "    import mlflow\n",
    "    ```\n",
    "  * Fetch runs as a DataFrame:\n",
    "\n",
    "    ```python\n",
    "    runs_df = mlflow.search_runs(experiment_names=[\"my-experiment\"])\n",
    "    print(runs_df.head())\n",
    "    ```\n",
    "  * Inspect specific parameters or metrics:\n",
    "\n",
    "    ```python\n",
    "    runs_df[[\"run_id\", \"metrics.rmse\", \"params.alpha\"]]\n",
    "    ```\n",
    "\n",
    "* **Why it matters:**\n",
    "  Programmatic access allows automated analysis, easy comparison, and integration with other data-processing tools.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Understand the Layout and Capabilities of the MLflow UI**\n",
    "\n",
    "* **What it means:**\n",
    "  Familiarizing yourself with MLflow's graphical user interface, which provides an easy way to browse and manage experiments and runs visually.\n",
    "\n",
    "* **Detailed Explanation:**\n",
    "  MLflow UI key sections:\n",
    "\n",
    "  * **Experiments:** Organized runs grouped by experiments.\n",
    "  * **Runs:** Detailed view of parameters, metrics, artifacts, tags, timestamps.\n",
    "  * **Comparison view:** Side-by-side comparison of multiple runs.\n",
    "  * **Artifacts tab:** Access to model files, plots, or logs stored during runs.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Visual interaction makes managing, comparing, and understanding your ML experiments much easier and faster, particularly when you have numerous runs or complex data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Sort, Filter, and Compare Experiments Visually**\n",
    "\n",
    "* **What it means:**\n",
    "  Using MLflow‚Äôs UI to quickly organize runs, filter by parameters or metrics, and visually compare experiment results.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * **Sorting:** Click on column headers in MLflow UI to sort runs by metric (e.g., lowest RMSE).\n",
    "  * **Filtering:** Enter queries in the filter box (e.g., `params.alpha = \"0.01\"`).\n",
    "  * **Visual comparisons:** Use the comparison feature to show multiple run metrics side-by-side graphically.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Easy visual sorting and filtering significantly simplify the task of identifying the best models or experiment conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Export Results for Offline Analysis**\n",
    "\n",
    "* **What it means:**\n",
    "  Downloading or exporting MLflow run data to files (e.g., CSV format) so you can analyze them outside the MLflow environment using tools like Excel, Pandas, or PowerBI.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Export runs programmatically to CSV:\n",
    "\n",
    "    ```python\n",
    "    runs_df = mlflow.search_runs(experiment_names=[\"my-experiment\"])\n",
    "    runs_df.to_csv(\"experiment_runs.csv\", index=False)\n",
    "    ```\n",
    "  * Download via MLflow UI:\n",
    "\n",
    "    * Use the \"Download CSV\" feature in the UI to get results manually.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Exporting data allows for deeper, customized analysis and easier sharing of experiment details with team members or stakeholders who prefer external analytics tools.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No runs found for experiment 'autologging-random-forest'.\n",
      "‚úÖ Run data exported to 'mlflow_run_comparison.csv'.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**To launch the MLflow Tracking UI locally:**\n",
       "\n",
       "```sh\n",
       "mlflow ui\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìì MLflow Experiment Analysis ‚Äî Jupyter Notebook Version\n",
    "\n",
    "# Step 1: Install MLflow if not already installed\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install --upgrade pip\n",
    "    !{sys.executable} -m pip install mlflow\n",
    "    import mlflow\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Step 2: Set or create experiment\n",
    "experiment_name = \"autologging-random-forest\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Step 3: Load all runs from the experiment\n",
    "runs_df = mlflow.search_runs(experiment_names=[experiment_name])\n",
    "\n",
    "# Step 4: Display key columns in a sorted DataFrame\n",
    "display_cols = [\n",
    "    \"run_id\",\n",
    "    \"metrics.training_score\",\n",
    "    \"metrics.test_rmse\",\n",
    "    \"params.max_depth\",\n",
    "]\n",
    "\n",
    "if not runs_df.empty:\n",
    "    summary_df = runs_df[display_cols].sort_values(\"metrics.test_rmse\")\n",
    "    print(\"Available runs in experiment (sorted by test_rmse):\")\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(f\"No runs found for experiment '{experiment_name}'.\")\n",
    "\n",
    "# Step 5: Export results to CSV\n",
    "output_csv = \"mlflow_run_comparison.csv\"\n",
    "runs_df.to_csv(output_csv, index=False)\n",
    "print(f\"‚úÖ Run data exported to '{output_csv}'.\")\n",
    "\n",
    "# Step 6: How to launch the MLflow UI (only from your terminal/Anaconda Prompt)\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(\"\"\"\n",
    "**To launch the MLflow Tracking UI locally:**\n",
    "\n",
    "```sh\n",
    "mlflow ui\n",
    "\"\"\" \n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl2Akd1ZpyFZ"
   },
   "source": [
    "## üìù Assessment: MLflow UI Deep Dive\n",
    "\n",
    "### üìò Multiple Choice (Choose the best answer)\n",
    "\n",
    "**1. What is the default URL to access the MLflow Tracking UI when launched locally?**   \n",
    "A. [http://localhost:3000](http://localhost:3000)   \n",
    "**B. [http://localhost:5000](http://localhost:5000)** ‚úÖ   \n",
    "C. [http://127.0.0.1:8888](http://127.0.0.1:8888)   \n",
    "D. [http://mlflow.localhost/ui](http://mlflow.localhost/ui)   \n",
    "\n",
    "---\n",
    "\n",
    "**2. Which MLflow command launches the UI from the command line?**   \n",
    "A. `mlflow run`   \n",
    "**B. `mlflow ui`** ‚úÖ   \n",
    "C. `mlflow open`   \n",
    "D. `mlflow start`   \n",
    "\n",
    "---\n",
    "\n",
    "**3. What can you do from the MLflow UI? (Select the most complete answer)**   \n",
    "A. View logged metrics only   \n",
    "B. Launch models into production directlyv   \n",
    "**C. Compare runs, download artifacts, inspect metrics/params, and register models** ‚úÖ   \n",
    "D. Execute Python scripts in runs   \n",
    "\n",
    "---\n",
    "\n",
    "**4. Which function allows you to retrieve past run data programmatically in MLflow?**   \n",
    "A. `mlflow.get_runs()`   \n",
    "B. `mlflow.experiments.get()`   \n",
    "**C. `mlflow.search_runs()`** ‚úÖ   \n",
    "D. `mlflow.list_models()`   \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Short Answer   \n",
    "\n",
    "**5. How does the MLflow UI help in comparing different model runs?**   \n",
    "*Explain sorting, metric charts, side-by-side comparisons.*   \n",
    "\n",
    "---\n",
    "\n",
    "**6. In what situations would exporting runs to CSV be useful?**   \n",
    "*Hint: Think about sharing, offline analysis, or integration with reporting tools.*   \n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Mini Project   \n",
    "\n",
    "**7. Task:**   \n",
    "After completing several training runs with different hyperparameters:   \n",
    "\n",
    "* Use `mlflow.search_runs()` to fetch all runs in an experiment   \n",
    "* Sort the runs based on a specific metric (e.g., `rmse`)   \n",
    "* Export the top 3 performing runs to a CSV file   \n",
    "* *(Optional)*: Open MLflow UI locally and take a screenshot of the run    comparison view   \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPk/xH5FmVhr7OkAVUnXaB/",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
